{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIFEBATCH Tutorial Notebook\n",
    "This notebook is meant as a demonstration of code used to extract data from the FIFEBATCH parquet files. The backend interaction with the parquet files is handled with the python module `duckdb`, which allows for SQL-style queries of the parquet data with a streaming-based SQL execution engine. This allows for the processing of large datasets that cannot fit in memory. \n",
    "\n",
    "## Required Imports\n",
    "This notebook uses several standard libraries for loading and processing data. The plotting library used exclusively here is `matplotlib`, though users should feel free to plot with whatever plotting library they wish.\n",
    "\n",
    "Each of the required libraries is present in `requirements.txt` and can be installed with one line:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Parquet?\n",
    "\n",
    "Parquet is a popular binary file format for storing large datasets. It is columnar in nature, which means that all data is stored by column and not by row. Practically, this means that the data belonging to a single field is stored in one consecutive region of memory for efficient access. This makes operations acting on an entire field very efficient. Additionally, the file format utilizes compression to conserve disk space.\n",
    "\n",
    "The Parquet format is popular for large datasets and used across many disciplines for these reasons.\n",
    "\n",
    "## What is DuckDB?\n",
    "\n",
    "DuckDB serves as a frontend for interacting with Parquet (and other file formats) files through standard SQL queries. This has the advantage of being quite formulaic and immediately familiar to people of a variety of backgrounds. The DuckDB database engine allows users to define a query to filter data before it even enters the Python code, thus making for efficient memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple DuckDB example\n",
    "\n",
    "With DuckDB, you can query Parquet files like they are database tables (think SQL):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = duckdb.sql(\"SELECT User FROM '../data/*.parquet'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create a \"Relation\" object called `rel` that essentially acts as a data access plan. It can be interpreted as \"I want all entries from the `User` field contained in the files matching the pattern `../data/*.parquet`.\" It's important to note that at this stage *no data has actually been accessed.* This is simply a description of the data that we want.\n",
    "\n",
    "A Relation is a plan for our query, and the user gets to decide how to materialize it:\n",
    "* `.df()` - a Pandas DataFrame\n",
    "* `.fetchnumpy()` - A NumPy array\n",
    "* `.fetch_arrow_table()` - Arrow table\n",
    "* `.fetch_record_batch()` - Arrow batches (for streaming)\n",
    "\n",
    "If a Parquet file has many rows (e.g. millions), pulling it all into memory at once can overwhelm the available system memory. This would manifest as a sluggish response in your computer and eventually a crashed Jupyter kernel (if using Jupyter notebooks). For now, we can fetch the result as a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['User'])\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data = rel.fetchnumpy()\n",
    "print(type(data))\n",
    "print(data.keys())\n",
    "print(type(data['User']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `data` object is a dictionary containing as key/values the fields you selected (here `User`) and the corresponding NumPy array for each. We can retrieve the array containing the `User` field and produce a count of the top ten users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['amakovec@fnal.gov', 'gm2pro@fnal.gov', 'laliaga@fnal.gov',\n",
       "       'imawby@fnal.gov', 'gputnam@fnal.gov', 'novapro@fnal.gov',\n",
       "       'ichong@fnal.gov', 'cmsgli@fnal.gov', 'icaruspro@fnal.gov',\n",
       "       'uboonepro@fnal.gov'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "users = data['User']\n",
    "unique, counts = np.unique(users, return_counts=True)\n",
    "top10 = np.argsort(counts)[-10:]\n",
    "display(unique[top10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
